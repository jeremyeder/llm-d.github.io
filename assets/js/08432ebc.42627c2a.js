"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[594],{7829:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"guide/Installation/quickstart","title":"Trying llm-d via the Quick Start installer","description":"For more information on llm-d, see the llm-d git repository here and website here.","source":"@site/docs/guide/Installation/quickstart.md","sourceDirName":"guide/Installation","slug":"/guide/Installation/quickstart","permalink":"/docs/guide/Installation/quickstart","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Quick Start installer"},"sidebar":"guideSidebar","previous":{"title":"Prerequisites for running llm-d","permalink":"/docs/guide/Installation/prerequisites"}}');var s=l(4848),t=l(8453);const r={sidebar_position:2,sidebar_label:"Quick Start installer"},a="Trying llm-d via the Quick Start installer",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Client Configuration",id:"client-configuration",level:2},{value:"Required tools",id:"required-tools",level:3},{value:"Required credentials and configuration",id:"required-credentials-and-configuration",level:3},{value:"Target Platforms",id:"target-platforms",level:3},{value:"Kubernetes",id:"kubernetes",level:4},{value:"OpenShift",id:"openshift",level:4},{value:"llm-d Installation",id:"llm-d-installation",level:2},{value:"Usage",id:"usage",level:3},{value:"Flags",id:"flags",level:3},{value:"Examples",id:"examples",level:2},{value:"Install llm-d on an Existing Kubernetes Cluster",id:"install-llm-d-on-an-existing-kubernetes-cluster",level:3},{value:"Install on OpenShift",id:"install-on-openshift",level:3},{value:"Validation",id:"validation",level:3},{value:"Bring Your Own Model",id:"bring-your-own-model",level:3},{value:"Metrics Collection",id:"metrics-collection",level:3},{value:"Accessing the Metrics UIs",id:"accessing-the-metrics-uis",level:4},{value:"Option 1: Port Forwarding (Default)",id:"option-1-port-forwarding-default",level:5},{value:"Option 2: Ingress (Optional)",id:"option-2-ingress-optional",level:5},{value:"Option 3: OpenShift",id:"option-3-openshift",level:5},{value:"Available Metrics",id:"available-metrics",level:4},{value:"Security Note",id:"security-note",level:4},{value:"Troubleshooting",id:"troubleshooting",level:3},{value:"Uninstall",id:"uninstall",level:3}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"trying-llm-d-via-the-quick-start-installer",children:"Trying llm-d via the Quick Start installer"})}),"\n",(0,s.jsxs)(n.p,{children:["For more information on llm-d, see the llm-d git repository ",(0,s.jsx)(n.a,{href:"https://github.com/llm-d/llm-d",children:"here"})," and website ",(0,s.jsx)(n.a,{href:"https://llmd.io",children:"here"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This guide will walk you through the steps to install and deploy llm-d on a Kubernetes cluster, using an opinionated flow in order to get up and running as quickly as possible."}),"\n",(0,s.jsx)(n.h2,{id:"client-configuration",children:"Client Configuration"}),"\n",(0,s.jsx)(n.h3,{id:"required-tools",children:"Required tools"}),"\n",(0,s.jsx)(n.p,{children:"Following prerequisite are required for the installer to work."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/mikefarah/yq?tab=readme-ov-file#install",children:"yq (mikefarah) \u2013 installation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://stedolan.github.io/jq/download/",children:"jq \u2013 download & install guide"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://git-scm.com/book/en/v2/Getting-Started-Installing-Git",children:"git \u2013 installation guide"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://helm.sh/docs/intro/install/",children:"Helm \u2013 quick-start install"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://kubectl.docs.kubernetes.io/installation/kustomize/",children:"Kustomize \u2013 official install docs"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/tools/install-kubectl/",children:"kubectl \u2013 install & setup"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"You can use the installer script that installs all the required dependencies.  Currently only Linux is supported."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Currently Linux only\r\n./install-deps.sh\n"})}),"\n",(0,s.jsx)(n.h3,{id:"required-credentials-and-configuration",children:"Required credentials and configuration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-deployer.git",children:"llm-d-deployer GitHub repo \u2013 clone here"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/settings/tokens",children:"ghcr.io Registry \u2013 credentials"}),' You must have a GitHub account and a "classic" personal access token with ',(0,s.jsx)(n.code,{children:"read:packages"})," access to the llm-d-deployer repository."]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://access.redhat.com/registry/",children:"Red Hat Registry \u2013 terms & access"})}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"HuggingFace HF_TOKEN"})," with download access for the model you want to use.  By default the sample application will use ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",children:"meta-llama/Llama-3.2-3B-Instruct"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["\u26a0\ufe0f Your Hugging Face account must have access to the model you want to use.  You may need to visit Hugging Face ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",children:"meta-llama/Llama-3.2-3B-Instruct"})," and\r\naccept the usage terms if you have not already done so."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Registry Authentication: The installer looks for an auth file in:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"~/.config/containers/auth.json\r\n# or\r\n~/.config/containers/config.json\n"})}),"\n",(0,s.jsx)(n.p,{children:"If not found, you can create one with the following commands:"}),"\n",(0,s.jsx)(n.p,{children:"Create with Docker:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker --config ~/.config/containers/ login ghcr.io\n"})}),"\n",(0,s.jsx)(n.p,{children:"Create with Podman:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"podman login ghcr.io --authfile ~/.config/containers/auth.json\n"})}),"\n",(0,s.jsx)(n.h3,{id:"target-platforms",children:"Target Platforms"}),"\n",(0,s.jsx)(n.h4,{id:"kubernetes",children:"Kubernetes"}),"\n",(0,s.jsxs)(n.p,{children:["This can be run on a minimum ec2 node type ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/g6e/",children:"g6e.12xlarge"})," (4xL40S 48GB but only 2 are used by default) to infer the model meta-llama/Llama-3.2-3B-Instruct that will get spun up."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["\u26a0\ufe0f If your cluster has no available GPUs, the ",(0,s.jsx)(n.strong,{children:"prefill"})," and ",(0,s.jsx)(n.strong,{children:"decode"})," pods will remain in ",(0,s.jsx)(n.strong,{children:"Pending"})," state."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Verify you have properly installed the container toolkit with the runtime of your choice."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Podman\r\npodman run --rm --security-opt=label=disable --device=nvidia.com/gpu=all ubuntu nvidia-smi\r\n# Docker\r\nsudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n"})}),"\n",(0,s.jsx)(n.h4,{id:"openshift",children:"OpenShift"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"OpenShift - This quickstart was tested on OpenShift 4.18. Older versions may work but have not been tested."}),"\n",(0,s.jsxs)(n.li,{children:["NVIDIA GPU Operator and NFD Operator - The installation instructions can be found ",(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/steps-overview.html",children:"here"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"NO Service Mesh or Istio installation as Istio CRDs will conflict with the gateway"}),"\n"]}),"\n",(0,s.jsx)("a",{name:"install"}),"\n",(0,s.jsx)(n.h2,{id:"llm-d-installation",children:"llm-d Installation"}),"\n",(0,s.jsxs)(n.p,{children:["The llm-d-deployer contains all the helm charts necessary to deploy llm-d. To facilitate the installation of the helm charts, the ",(0,s.jsx)(n.code,{children:"llmd-installer.sh"})," script is provided. This script will populate the necessary manifests in the ",(0,s.jsx)(n.code,{children:"manifests"})," directory.\r\nAfter this, it will apply all the manifests in order to bring up the cluster."]}),"\n",(0,s.jsx)(n.p,{children:"The llmd-installer.sh script aims to simplify the installation of llm-d using the llm-d-deployer as it's main function.  It scripts as many of the steps as possible to make the installation process more streamlined.  This includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Installing the GAIE infrastructure"}),"\n",(0,s.jsx)(n.li,{children:"Creating the namespace with any special configurations"}),"\n",(0,s.jsx)(n.li,{children:"Creating the pull secret to download the images"}),"\n",(0,s.jsx)(n.li,{children:"Creating the model service CRDs"}),"\n",(0,s.jsx)(n.li,{children:"Applying the helm charts"}),"\n",(0,s.jsx)(n.li,{children:"Deploying the sample app (model service)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"It also supports uninstalling the llm-d infrastructure and the sample app."}),"\n",(0,s.jsxs)(n.p,{children:["Before proceeding with the installation, ensure you have completed the prerequisites and are able to issue kubectl commands to your cluster by configuring your ",(0,s.jsx)(n.code,{children:"~/.kube/config"})," file or by using the ",(0,s.jsx)(n.code,{children:"oc login"})," command."]}),"\n",(0,s.jsx)(n.h3,{id:"usage",children:"Usage"}),"\n",(0,s.jsxs)(n.p,{children:["The installer needs to be run from the ",(0,s.jsx)(n.code,{children:"llm-d-deployer/quickstart"})," directory."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./llmd-installer.sh [OPTIONS]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"flags",children:"Flags"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Flag"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Example"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--hf-token TOKEN"})}),(0,s.jsxs)(n.td,{children:["HuggingFace API token (or set ",(0,s.jsx)(n.code,{children:"HF_TOKEN"})," env var)"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:'./llmd-installer.sh --hf-token "abc123"'})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--auth-file PATH"})}),(0,s.jsx)(n.td,{children:"Path to your registry auth file ig not in one of the two listed files in the auth section of the readme"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"./llmd-installer.sh --auth-file ~/.config/containers/auth.json"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--storage-size SIZE"})}),(0,s.jsx)(n.td,{children:"Size of storage volume (default: 7Gi)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"./llmd-installer.sh --storage-size 15Gi"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--skip-download-model"})}),(0,s.jsx)(n.td,{children:"Skip downloading the model to PVC if modelArtifactURI is pvc based"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"./llmd-installer.sh --skip-download-model"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--storage-class CLASS"})}),(0,s.jsx)(n.td,{children:"Storage class to use (default: efs-sc)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"./llmd-installer.sh --storage-class ocs-storagecluster-cephfs"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--namespace NAME"})}),(0,s.jsxs)(n.td,{children:["Kubernetes namespace to use (default: ",(0,s.jsx)(n.code,{children:"llm-d"}),")"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"./llmd-installer.sh --namespace foo"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--values-file NAME"})}),(0,s.jsx)(n.td,{children:"Absolute path to a Helm values.yaml file (default: llm-d-deployer/charts/llm-d/values.yaml)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"./llmd-installer.sh --values-file /path/to/values.yaml"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--uninstall"})}),(0,s.jsx)(n.td,{children:"Uninstall llm-d and cleanup resources"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"./llmd-installer.sh --uninstall"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"--disable-metrics-collection"})}),(0,s.jsx)(n.td,{children:"Disable metrics collection (Prometheus will not be installed)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"./llmd-installer.sh --disable-metrics-collection"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:"-h"}),", ",(0,s.jsx)(n.code,{children:"--help"})]}),(0,s.jsx)(n.td,{children:"Show help and exit"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"./llmd-installer.sh --help"})})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.h3,{id:"install-llm-d-on-an-existing-kubernetes-cluster",children:"Install llm-d on an Existing Kubernetes Cluster"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'export HF_TOKEN="your-token"\r\n./llmd-installer.sh\n'})}),"\n",(0,s.jsx)(n.h3,{id:"install-on-openshift",children:"Install on OpenShift"}),"\n",(0,s.jsx)(n.p,{children:"Before running the installer, ensure you have logged into the cluster.  For example:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"oc login --token=sha256~yourtoken --server=https://api.yourcluster.com:6443\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'export HF_TOKEN="your-token"\r\n./llmd-installer.sh\n'})}),"\n",(0,s.jsx)("a",{name:"explore"}),"\n",(0,s.jsx)(n.h3,{id:"validation",children:"Validation"}),"\n",(0,s.jsxs)(n.p,{children:["The inference-gateway serves as the HTTP ingress point for all inference requests in our deployment.\r\nIt\u2019s implemented as a Kubernetes Gateway (",(0,s.jsx)(n.code,{children:"gateway.networking.k8s.io/v1"}),") using either kgateway or istio as the\r\ngatewayClassName, and sits in front of your inference pods to handle path-based routing, load balancing, retries,\r\nand metrics. This example validates that the gateway itself is routing your completion requests correctly.\r\nYou can execute the ",(0,s.jsx)(n.a,{href:"test-request.sh",children:(0,s.jsx)(n.code,{children:"test-request.sh"})})," script to test on the cluster."]}),"\n",(0,s.jsx)(n.p,{children:"In addition, if you're using an OpenShift Cluster or have created an ingress, you can test the endpoint from an external location."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'INGRESS_ADDRESS=$(kubectl get ingress -n "$NAMESPACE" | tail -n1 | awk \'{print $3}\')\r\n\r\ncurl -sS -X GET "http://${INGRESS_ADDRESS}/v1/models" \\\r\n    -H \'accept: application/json\' \\\r\n    -H \'Content-Type: application/json\'\r\n\r\nMODEL_ID=meta-llama/Llama-3.2-3B-Instruct\r\n\r\ncurl -sS -X POST "http://${INGRESS_ADDRESS}/v1/completions" \\\r\n  -H \'accept: application/json\' \\\r\n  -H \'Content-Type: application/json\' \\\r\n  -d \'{\r\n    "model":"\'"$MODEL_ID"\'",\r\n    "prompt": "You are a helpful AI assistant. Please introduce yourself in one sentence.",\r\n  }\'\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:'If you receive an error indicating PodSecurity "restricted" violations when running the smoke-test script, you\r\nneed to remove the restrictive PodSecurity labels from the namespace. Once these labels are removed, re-run the\r\nscript and it should proceed without PodSecurity errors.\r\nRun the following command:'}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl label namespace <NAMESPACE> \\\r\n  pod-security.kubernetes.io/warn- \\\r\n  pod-security.kubernetes.io/warn-version- \\\r\n  pod-security.kubernetes.io/audit- \\\r\n  pod-security.kubernetes.io/audit-version-\n"})}),"\n",(0,s.jsx)(n.h3,{id:"bring-your-own-model",children:"Bring Your Own Model"}),"\n",(0,s.jsxs)(n.p,{children:["There is a default sample application that by loads ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",children:(0,s.jsx)(n.code,{children:"meta-llama/Llama-3.2-3B-Instruct"})}),"\r\nbased on the sample application ",(0,s.jsx)(n.a,{href:"../charts/llm-d/values.yaml",children:"values.yaml"})," file. If you want to swap that model out with\r\nanother ",(0,s.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/models/supported_models.html",children:"vllm compatible model"}),". Simply modify the\r\nvalues file with the model you wish to run."]}),"\n",(0,s.jsxs)(n.p,{children:["Here is an example snippet of the default model values being replaced with\r\n",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct",children:(0,s.jsx)(n.code,{children:"meta-llama/Llama-3.2-1B-Instruct"})}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'  model:\r\n    # -- Fully qualified pvc URI: pvc://<pvc-name>/<model-path>\r\n    modelArtifactURI: pvc://llama-3.2-1b-instruct-pvc/models/meta-llama/Llama-3.2-1B-Instruct\r\n\r\n    # # -- Fully qualified hf URI: pvc://<pvc-name>/<model-path>\r\n    # modelArtifactURI: hf://meta-llama/Llama-3.2-3B-Instruct\r\n\r\n    # -- Name of the model\r\n    modelName: "Llama-3.2-1B-Instruct"\r\n\r\n    # -- Aliases to the Model named vllm will serve with\r\n    servedModelNames: []\r\n\r\n    auth:\r\n      # -- HF token auth config via k8s secret. Required if using hf:// URI or not using pvc:// URI with `--skip-download-model` in quickstart\r\n      hfToken:\r\n        # -- If the secret should be created or one already exists\r\n        create: true\r\n        # -- Name of the secret to create to store your huggingface token\r\n        name: llm-d-hf-token\r\n        # -- Value of the token. Do not set this but use `envsubst` in conjunction with the helm chart\r\n        key: HF_TOKEN\n'})}),"\n",(0,s.jsx)(n.h3,{id:"metrics-collection",children:"Metrics Collection"}),"\n",(0,s.jsxs)(n.p,{children:["llm-d includes built-in support for metrics collection using Prometheus and Grafana. This feature is enabled by default but can be disabled using the\r\n",(0,s.jsx)(n.code,{children:"--disable-metrics-collection"})," flag during installation. In OpenShift, llm-d applies ServiceMonitors for llm-d components that trigger Prometheus\r\nscrape targets for the built-in user workload monitoring Prometheus stack."]}),"\n",(0,s.jsx)(n.h4,{id:"accessing-the-metrics-uis",children:"Accessing the Metrics UIs"}),"\n",(0,s.jsxs)(n.p,{children:["If running in OpenShift, skip to ",(0,s.jsx)(n.a,{href:"#option-3-openshift",children:"Option 3: OpenShift"}),"."]}),"\n",(0,s.jsx)(n.h5,{id:"option-1-port-forwarding-default",children:"Option 1: Port Forwarding (Default)"}),"\n",(0,s.jsx)(n.p,{children:"Once installed, you can access the metrics UIs through port-forwarding:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Prometheus UI (port 9090):"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward -n llm-d-monitoring --address 0.0.0.0 svc/prometheus-kube-prometheus-prometheus 9090:9090\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Grafana UI (port 3000):"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward -n llm-d-monitoring --address 0.0.0.0 svc/prometheus-grafana 3000:80\n"})}),"\n",(0,s.jsx)(n.p,{children:"Access the UIs at:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Prometheus: ",(0,s.jsx)(n.code,{children:"<http://YOUR_IP:9090>"})]}),"\n",(0,s.jsxs)(n.li,{children:["Grafana: ",(0,s.jsx)(n.code,{children:"<http://YOUR_IP:3000> (default credentials: admin/admin)"})]}),"\n"]}),"\n",(0,s.jsx)(n.h5,{id:"option-2-ingress-optional",children:"Option 2: Ingress (Optional)"}),"\n",(0,s.jsx)(n.p,{children:"For production environments, you can configure ingress for both Prometheus and Grafana. Add the following to your values.yaml:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"prometheus:\r\n  ingress:\r\n    enabled: true\r\n    annotations:\r\n      kubernetes.io/ingress.class: nginx\r\n    hosts:\r\n      - prometheus.your-domain.com\r\n    tls:\r\n      - secretName: prometheus-tls\r\n        hosts:\r\n          - prometheus.your-domain.com\r\n\r\ngrafana:\r\n  ingress:\r\n    enabled: true\r\n    annotations:\r\n      kubernetes.io/ingress.class: nginx\r\n    hosts:\r\n      - grafana.your-domain.com\r\n    tls:\r\n      - secretName: grafana-tls\r\n        hosts:\r\n          - grafana.your-domain.com\n"})}),"\n",(0,s.jsx)(n.h5,{id:"option-3-openshift",children:"Option 3: OpenShift"}),"\n",(0,s.jsx)(n.p,{children:"If you're using OpenShift with user workload monitoring enabled, you can access the metrics through the OpenShift console:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Navigate to the OpenShift console"}),"\n",(0,s.jsx)(n.li,{children:'In the left navigation bar, click on "Observe"'}),"\n",(0,s.jsxs)(n.li,{children:["You can access:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Metrics: Click on "Metrics" to view and query metrics using the built-in Prometheus UI'}),"\n",(0,s.jsx)(n.li,{children:'Targets: Click on "Targets" to see all monitored endpoints and their status'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The metrics are automatically integrated into the OpenShift monitoring stack, providing a seamless experience for viewing and analyzing your llm-d metrics.\r\nThe llm-d-deployer does not install Grafana in OpenShift, but it's recommended that users install Grafana to view metrics and import dashboards."}),"\n",(0,s.jsxs)(n.p,{children:["Follow the ",(0,s.jsx)(n.a,{href:"https://github.com/llm-d/llm-d/blob/dev/observability/openshift/README.md#step-2-set-up-grafana-optional",children:"OpenShift Grafana setup guide"}),"\r\nThe guide includes manifests to install the following:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Grafana instance"}),"\n",(0,s.jsx)(n.li,{children:"Grafana Prometheus datasource from user workload monitoring stack"}),"\n",(0,s.jsx)(n.li,{children:"Grafana llm-d dashboard"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"available-metrics",children:"Available Metrics"}),"\n",(0,s.jsx)(n.p,{children:"The metrics collection includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Model inference performance metrics"}),"\n",(0,s.jsx)(n.li,{children:"Request latency and throughput"}),"\n",(0,s.jsx)(n.li,{children:"Resource utilization (CPU, memory, GPU)"}),"\n",(0,s.jsx)(n.li,{children:"Cache hit/miss rates"}),"\n",(0,s.jsx)(n.li,{children:"Error rates and types"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"security-note",children:"Security Note"}),"\n",(0,s.jsx)(n.p,{children:"When running in a cloud environment (like EC2), make sure to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Configure your security groups to allow inbound traffic on ports 9090 and 3000 (if using port-forwarding)"}),"\n",(0,s.jsxs)(n.li,{children:["Use the ",(0,s.jsx)(n.code,{children:"--address 0.0.0.0"})," flag with port-forward to allow external access"]}),"\n",(0,s.jsx)(n.li,{children:"Consider setting up proper authentication for production environments"}),"\n",(0,s.jsx)(n.li,{children:"If using ingress, ensure proper TLS configuration and authentication"}),"\n",(0,s.jsx)(n.li,{children:"For OpenShift, consider using the built-in OAuth integration for Grafana"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.p,{children:"The various images can take some time to download depending on your connectivity. Watching events\r\nand logs of the prefill and decode pods is a good place to start."}),"\n",(0,s.jsx)(n.h3,{id:"uninstall",children:"Uninstall"}),"\n",(0,s.jsx)(n.p,{children:"This will remove llm-d resources from the cluster. This is useful, especially for test/dev if you want to\r\nmake a change, simply uninstall and then run the installer again with any changes you make."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./llmd-installer.sh --uninstall\n"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,l)=>{l.d(n,{R:()=>r,x:()=>a});var i=l(6540);const s={},t=i.createContext(s);function r(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);