"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[2915],{5163:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"architecture/Component Architecture/inf-extension","title":"inf-extension","description":"Go Report Card","source":"@site/docs/architecture/Component Architecture/02_inf-extension.md","sourceDirName":"architecture/Component Architecture","slug":"/architecture/Component Architecture/inf-extension","permalink":"/docs/architecture/Component Architecture/inf-extension","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Inference Extension"},"sidebar":"structureSidebar","previous":{"title":"Deployer Architecture","permalink":"/docs/architecture/Component Architecture/deployer"},"next":{"title":"Inference Scheduler","permalink":"/docs/architecture/Component Architecture/inf-scheduler"}}');var r=t(4848),s=t(8453);const o={sidebar_position:2,sidebar_label:"Inference Extension"},a="Gateway API Inference Extension",c={},l=[{value:"Concepts and Definitions",id:"concepts-and-definitions",level:2},{value:"Technical Overview",id:"technical-overview",level:2},{value:"Status",id:"status",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Roadmap",id:"roadmap",level:2},{value:"End-to-End Tests",id:"end-to-end-tests",level:2},{value:"Contributing",id:"contributing",level:2},{value:"Code of conduct",id:"code-of-conduct",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://goreportcard.com/report/sigs.k8s.io/gateway-api-inference-extension",children:(0,r.jsx)(n.img,{src:"https://goreportcard.com/badge/sigs.k8s.io/gateway-api-inference-extension",alt:"Go Report Card"})}),"\r\n",(0,r.jsx)(n.a,{href:"https://pkg.go.dev/sigs.k8s.io/gateway-api-inference-extension",children:(0,r.jsx)(n.img,{src:"https://pkg.go.dev/badge/sigs.k8s.io/gateway-api-inference-extension.svg",alt:"Go Reference"})}),"\r\n[",(0,r.jsx)(n.img,{src:"https://img.shields.io/github/license/kubernetes-sigs/gateway-api-inference-extension/LICENSE",alt:"License"}),"]"]}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"gateway-api-inference-extension",children:"Gateway API Inference Extension"})}),"\n",(0,r.jsxs)(n.p,{children:["Gateway API Inference Extension optimizes self-hosting Generative Models on Kubernetes.\r\nThis is achieved by leveraging Envoy's ",(0,r.jsx)(n.a,{href:"https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/ext_proc_filter",children:"External Processing"})," (ext-proc) to extend any gateway that supports both ext-proc and ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/gateway-api",children:"Gateway API"})," into an ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"#concepts-and-definitions",children:"inference gateway"})}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"concepts-and-definitions",children:"Concepts and Definitions"}),"\n",(0,r.jsx)(n.p,{children:"The following specific terms to this project:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference Gateway (IGW)"}),": A proxy/load-balancer which has been coupled with an\r\n",(0,r.jsx)(n.code,{children:"Endpoint Picker"}),". It provides optimized routing and load balancing for\r\nserving Kubernetes self-hosted generative Artificial Intelligence (AI)\r\nworkloads. It simplifies the deployment, management, and observability of AI\r\ninference workloads."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference Scheduler"}),": An extendable component that makes decisions about which endpoint is optimal (best cost /\r\nbest performance) for an inference request based on ",(0,r.jsx)(n.code,{children:"Metrics and Capabilities"}),"\r\nfrom ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/gateway-api-inference-extension/tree/main/docs/proposals/003-model-server-protocol/README.md",children:"Model Serving"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Metrics and Capabilities"}),": Data provided by model serving platforms about\r\nperformance, availability and capabilities to optimize routing. Includes\r\nthings like ",(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/stable/design/v1/prefix_caching.html",children:"Prefix Cache"})," status or ",(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/stable/features/lora.html",children:"LoRA Adapters"})," availability."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Endpoint Picker(EPP)"}),": An implementation of an ",(0,r.jsx)(n.code,{children:"Inference Scheduler"})," with additional Routing, Flow, and Request Control layers to allow for sophisticated routing strategies. Additional info on the architecture of the EPP ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/gateway-api-inference-extension/tree/main/docs/proposals/0683-epp-architecture-proposal",children:"here"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The following are key industry terms that are important to understand for\r\nthis project:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model"}),": A generative AI model that has learned patterns from data and is\r\nused for inference. Models vary in size and architecture, from smaller\r\ndomain-specific models to massive multi-billion parameter neural networks that\r\nare optimized for diverse language tasks."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference"}),": The process of running a generative AI model, such as a large\r\nlanguage model, diffusion model etc, to generate text, embeddings, or other\r\noutputs from input data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model server"}),": A service (in our case, containerized) responsible for\r\nreceiving inference requests and returning predictions from a model."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accelerator"}),": specialized hardware, such as Graphics Processing Units\r\n(GPUs) that can be attached to Kubernetes nodes to speed up computations,\r\nparticularly for training and inference tasks."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["For deeper insights and more advanced concepts, refer to our ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/gateway-api-inference-extension/tree/main/docs/proposals",children:"proposals"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"technical-overview",children:"Technical Overview"}),"\n",(0,r.jsxs)(n.p,{children:["This extension upgrades an ",(0,r.jsx)(n.a,{href:"https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/ext_proc_filter",children:"ext-proc"})," capable proxy or gateway - such as Envoy Gateway, kGateway, or the GKE Gateway - to become an ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"#concepts-and-definitions",children:"inference gateway"})})," - supporting inference platform teams self-hosting Generative Models (with a current focus on large language models) on Kubernetes. This integration makes it easy to expose and control access to your local ",(0,r.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/chat",children:"OpenAI-compatible chat completion endpoints"})," to other workloads on or off cluster, or to integrate your self-hosted models alongside model-as-a-service providers in a higher level ",(0,r.jsx)(n.strong,{children:"AI Gateway"})," like LiteLLM, Solo AI Gateway, or Apigee."]}),"\n",(0,r.jsx)(n.p,{children:"The Inference Gateway:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Improves the tail latency and throughput of LLM completion requests against Kubernetes-hosted model servers using an extensible request scheduling alogrithm that is kv-cache and request cost aware, avoiding evictions or queueing as load increases"}),"\n",(0,r.jsxs)(n.li,{children:["Provides ",(0,r.jsx)(n.a,{href:"https://gateway-api-inference-extension.sigs.k8s.io/concepts/api-overview/",children:"Kubernetes-native declarative APIs"})," to route client model names to use-case specific LoRA adapters and control incremental rollout of new adapter versions, A/B traffic splitting, and safe blue-green base model and model server upgrades"]}),"\n",(0,r.jsx)(n.li,{children:"Adds end to end observability around service objective attainment"}),"\n",(0,r.jsx)(n.li,{children:"Ensures operational guardrails between different client model names, allowing a platform team to safely serve many different GenAI workloads on the same pool of shared foundation model servers for higher utilization and fewer required accelerators"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Architecture Diagram",src:t(9887).A+"",width:"1119",height:"782"})}),"\n",(0,r.jsxs)(n.p,{children:["It currently requires a version of vLLM that supports the necessary metrics to predict traffic load which is defined in the ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/gateway-api-inference-extension/tree/main/docs/proposals/003-model-server-protocol",children:"model server protocol"}),".  Support for Google's Jetstream, nVidia Triton, text-generation-inference, and SGLang is coming soon."]}),"\n",(0,r.jsx)(n.h2,{id:"status",children:"Status"}),"\n",(0,r.jsxs)(n.p,{children:["This project is ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/tag/v0.3.0",children:"alpha (0.3 release)"}),".  It should not be used in production yet."]}),"\n",(0,r.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsxs)(n.p,{children:["Follow our ",(0,r.jsx)(n.a,{href:"https://gateway-api-inference-extension.sigs.k8s.io/guides/",children:"Getting Started Guide"})," to get the inference-extension up and running on your cluster!"]}),"\n",(0,r.jsxs)(n.p,{children:["See our website at ",(0,r.jsx)(n.a,{href:"https://gateway-api-inference-extension.sigs.k8s.io/",children:"https://gateway-api-inference-extension.sigs.k8s.io/"})," for detailed API documentation on leveraging our Kubernetes-native declarative APIs"]}),"\n",(0,r.jsx)(n.h2,{id:"roadmap",children:"Roadmap"}),"\n",(0,r.jsx)(n.p,{children:"As Inference Gateway builds towards a GA release. We will continue to expand our capabilities, namely:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Prefix-cache aware load balancing with interfaces for remote caches"}),"\n",(0,r.jsx)(n.li,{children:"Recommended LoRA adapter pipeline for automated rollout"}),"\n",(0,r.jsx)(n.li,{children:"Fairness and priority between workloads within the same criticality band"}),"\n",(0,r.jsx)(n.li,{children:"HPA support for autoscaling on aggregate metrics derived from the load balancer"}),"\n",(0,r.jsx)(n.li,{children:"Support for large multi-modal inputs and outputs"}),"\n",(0,r.jsx)(n.li,{children:"Support for other GenAI model types (diffusion and other non-completion protocols)"}),"\n",(0,r.jsx)(n.li,{children:"Heterogeneous accelerators - serve workloads on multiple types of accelerator using latency and request cost-aware load balancing"}),"\n",(0,r.jsx)(n.li,{children:"Disaggregated serving support with independently scaling pools"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"end-to-end-tests",children:"End-to-End Tests"}),"\n",(0,r.jsxs)(n.p,{children:["Follow this link to the ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/main/test/e2e/epp/README.md",children:"e2e README"})," to learn more about running the inference-extension end-to-end test suite on your cluster."]}),"\n",(0,r.jsx)(n.h2,{id:"contributing",children:"Contributing"}),"\n",(0,r.jsxs)(n.p,{children:["Our community meeting is weekly at Thursday 10AM PDT (",(0,r.jsx)(n.a,{href:"https://zoom.us/j/9955436256?pwd=Z2FQWU1jeDZkVC9RRTN4TlZyZTBHZz09",children:"Zoom"}),", ",(0,r.jsx)(n.a,{href:"https://www.google.com/url?q=https://docs.google.com/document/d/1frfPE5L1sI3737rdQV04IcDGeOcGJj2ItjMg6z2SRH0/edit?usp%3Dsharing&sa=D&source=calendar&usd=2&usg=AOvVaw1pUVy7UN_2PMj8qJJcFm1U",children:"Meeting Notes"}),")."]}),"\n",(0,r.jsxs)(n.p,{children:["We currently utilize the ",(0,r.jsx)(n.a,{href:"https://kubernetes.slack.com/?redir=%2Fmessages%2Fwg-serving",children:"#wg-serving"})," slack channel for communications."]}),"\n",(0,r.jsxs)(n.p,{children:["Contributions are readily welcomed, follow the ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/main/docs/dev.md",children:"dev guide"})," to start contributing!"]}),"\n",(0,r.jsx)(n.h3,{id:"code-of-conduct",children:"Code of conduct"}),"\n",(0,r.jsxs)(n.p,{children:["Participation in the Kubernetes community is governed by the ",(0,r.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/gateway-api-inference-extension/blob/main/code-of-conduct.md",children:"Kubernetes Code of Conduct"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var i=t(6540);const r={},s=i.createContext(r);function o(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(s.Provider,{value:n},e.children)}},9887:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/inference-gateway-architecture-3229ffa77f3e36c54f75f76aae2b7f5e.svg"}}]);