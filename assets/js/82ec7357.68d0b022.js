"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[4155],{4179:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"architecture/Component Architecture/inf-simulator","title":"vLLM Simulator","description":"To help with development and testing we have developed a light weight vLLM simulator. It does not truly","source":"@site/docs/architecture/Component Architecture/02_inf-simulator.md","sourceDirName":"architecture/Component Architecture","slug":"/architecture/Component Architecture/inf-simulator","permalink":"/docs/architecture/Component Architecture/inf-simulator","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Inference Simulator"},"sidebar":"structureSidebar","previous":{"title":"Deployer Architecture","permalink":"/docs/architecture/Component Architecture/deployer"},"next":{"title":"Inference Scheduler","permalink":"/docs/architecture/Component Architecture/inf-scheduler"}}');var r=i(4848),t=i(8453);const l={sidebar_position:2,sidebar_label:"Inference Simulator"},o="vLLM Simulator",d={},c=[{value:"Limitations",id:"limitations",level:2},{value:"Command line parameters",id:"command-line-parameters",level:2},{value:"Working with docker image",id:"working-with-docker-image",level:2},{value:"Building",id:"building",level:3},{value:"Running",id:"running",level:3},{value:"Standalone testing",id:"standalone-testing",level:2},{value:"Building",id:"building-1",level:3},{value:"Running",id:"running-1",level:3}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components},{Details:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vllm-simulator",children:"vLLM Simulator"})}),"\n",(0,r.jsx)(n.p,{children:"To help with development and testing we have developed a light weight vLLM simulator. It does not truly\r\nrun inference, but it does emulate responses to the HTTP REST endpoints of vLLM.\r\nCurrently it supports partial OpenAI-compatible API:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"/v1/chat/completions"}),"\n",(0,r.jsx)(n.li,{children:"/v1/completions"}),"\n",(0,r.jsx)(n.li,{children:"/v1/models"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In addition, it supports a subset of vLLM's Prometheus metrics. These metrics are exposed via the /metrics HTTP REST endpoint. Currently supported are the following metrics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["vllm",":lora_requests_info"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The simulated inferense has no connection with the model and LoRA adapters specified in the command line parameters. The /v1/models endpoint returns simulated results based on those same command line parameters."}),"\n",(0,r.jsx)(n.p,{children:"The simulator supports two modes of operation:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"echo"})," mode: the response contains the same text that was received in the request. For ",(0,r.jsx)(n.code,{children:"/v1/chat/completions"})," the last message for the role=",(0,r.jsx)(n.code,{children:"user"})," is used."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"random"})," mode: the response is randomly chosen from a set of pre-defined sentences."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Timing of the response is defined by two parameters: ",(0,r.jsx)(n.code,{children:"time-to-first-token"})," and ",(0,r.jsx)(n.code,{children:"inter-token-latency"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["For a request with ",(0,r.jsx)(n.code,{children:"stream=true"}),": ",(0,r.jsx)(n.code,{children:"time-to-first-token"})," defines the delay before the first token is returned, ",(0,r.jsx)(n.code,{children:"inter-token-latency"})," defines the delay between subsequent tokens in the stream."]}),"\n",(0,r.jsxs)(n.p,{children:["For a requst with ",(0,r.jsx)(n.code,{children:"stream=false"}),": the response is returned after delay of ",(0,r.jsx)(n.code,{children:"<time-to-first-token> + (<inter-token-latency> * (<number_of_output_tokens> - 1))"})]}),"\n",(0,r.jsx)(n.p,{children:"It can be run standalone or in a Pod for testing under packages such as Kind."}),"\n",(0,r.jsx)(n.h2,{id:"limitations",children:"Limitations"}),"\n",(0,r.jsx)(n.p,{children:"API responses contains a subset of the fields provided by the OpenAI API."}),"\n",(0,r.jsxs)(i,{children:[(0,r.jsx)("summary",{children:"Click to show the structure of requests/responses"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v1/chat/completions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"request"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"stream"}),"\n",(0,r.jsx)(n.li,{children:"model"}),"\n",(0,r.jsxs)(n.li,{children:["messages","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"role"}),"\n",(0,r.jsx)(n.li,{children:"content"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"response"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"id"}),"\n",(0,r.jsx)(n.li,{children:"created"}),"\n",(0,r.jsx)(n.li,{children:"model"}),"\n",(0,r.jsxs)(n.li,{children:["choices","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"index"}),"\n",(0,r.jsx)(n.li,{children:"finish_reason"}),"\n",(0,r.jsx)(n.li,{children:"message"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v1/completions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"request"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"stream"}),"\n",(0,r.jsx)(n.li,{children:"model"}),"\n",(0,r.jsx)(n.li,{children:"prompt"}),"\n",(0,r.jsx)(n.li,{children:"max_tokens (for future usage)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"response"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"id"}),"\n",(0,r.jsx)(n.li,{children:"created"}),"\n",(0,r.jsx)(n.li,{children:"model"}),"\n",(0,r.jsxs)(n.li,{children:["choices","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"text"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v1/models"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"response"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"object (list)"}),"\n",(0,r.jsxs)(n.li,{children:["data","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"id"}),"\n",(0,r.jsx)(n.li,{children:"object (model)"}),"\n",(0,r.jsx)(n.li,{children:"created"}),"\n",(0,r.jsx)(n.li,{children:"owned_by"}),"\n",(0,r.jsx)(n.li,{children:"root"}),"\n",(0,r.jsx)(n.li,{children:"parent"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]})]}),"\n",(0,r.jsx)("br",{}),"\n",(0,r.jsxs)(n.p,{children:["For more details see the ",(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/stable/getting_started/quickstart.html#openai-completions-api-with-vllm",children:"vLLM documentation"})]}),"\n",(0,r.jsx)(n.h2,{id:"command-line-parameters",children:"Command line parameters"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"port"}),": the port the simulator listents on, mandatory"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"model"}),": the currently 'loaded' model, mandatory"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"lora"}),": a list of available LoRA adapters, separated by commas, optional, by default empty"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"mode"}),": the simulator mode, optional, by default ",(0,r.jsx)(n.code,{children:"random"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"echo"}),": returns the same text that was sent in the request"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"random"}),": returns a sentence chosen at random from a set of pre-defined sentences"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"time-to-first-token"}),": the time to the first token (in milliseconds), optional, by default zero"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"inter-token-latency"}),": the time to 'generate' each additional token (in milliseconds), optional, by default zero"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"max-loras"}),": maximum number of LoRAs in a single batch, optional, default is one"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"max-cpu-loras"}),": maximum number of LoRAs to store in CPU memory, optional, must be >= than max_loras, default is max_loras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"max-running-requests"}),": maximum number of inference requests that could be processed at the same time"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"working-with-docker-image",children:"Working with docker image"}),"\n",(0,r.jsx)(n.h3,{id:"building",children:"Building"}),"\n",(0,r.jsx)(n.p,{children:"To build a Docker image of the vLLM Simulator, run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"make build-llm-d-inference-sim-image\n"})}),"\n",(0,r.jsx)(n.h3,{id:"running",children:"Running"}),"\n",(0,r.jsx)(n.p,{children:"To run the vLLM Simulator image under Docker, run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'docker run --rm --publish 8000:8000 ai-aware-router/llm-d-inference-sim:0.0.1 /ai-aware-router/llm-d-inference-sim  --port 8000 --model "Qwen/Qwen2.5-1.5B-Instruct" --lora "tweet-summary-0,tweet-summary-1"\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," The above command exposes the simulator on port 8000, and serves the Qwen/Qwen2.5-1.5B-Instruct model."]}),"\n",(0,r.jsx)(n.h2,{id:"standalone-testing",children:"Standalone testing"}),"\n",(0,r.jsx)(n.h3,{id:"building-1",children:"Building"}),"\n",(0,r.jsx)(n.p,{children:"To build the vLLM simulator, run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"make build-llm-d-inference-sim\n"})}),"\n",(0,r.jsx)(n.h3,{id:"running-1",children:"Running"}),"\n",(0,r.jsx)(n.p,{children:"To run the router in a standalone test environment, run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"./bin/llm-d-inference-sim --model my_model --port 8000\n"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function l(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);