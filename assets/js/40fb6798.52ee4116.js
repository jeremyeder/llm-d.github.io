"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[3825],{5755:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>n,toc:()=>d});const n=JSON.parse('{"id":"architecture/faq","title":"Frequently Asked Questions","description":"Q: What is llm-d?","source":"@site/docs/architecture/faq.md","sourceDirName":"architecture","slug":"/architecture/faq","permalink":"/webdocs/docs/architecture/faq","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"label":"FAQ"},"sidebar":"structureSidebar","previous":{"title":"Overview of llm-d architecture","permalink":"/webdocs/docs/architecture/architecture"},"next":{"title":"Deployer Architecture","permalink":"/webdocs/docs/architecture/Components/deployer"}}');var r=i(4848),a=i(8453);const s={sidebar_position:3,label:"FAQ"},o="Frequently Asked Questions",l={},d=[{value:"Q: What is llm-d?",id:"q-what-is-llm-d",level:2},{value:"Q: What is the goal of the llm-d project?",id:"q-what-is-the-goal-of-the-llm-d-project",level:2},{value:"Q: What is the engineering design philosophy for the project?",id:"q-what-is-the-engineering-design-philosophy-for-the-project",level:2},{value:"Q: What are the key features of llm-d being presented at Red Hat Summit?",id:"q-what-are-the-key-features-of-llm-d-being-presented-at-red-hat-summit",level:2},{value:"Q: Is any Red Hat subscription required to use llm-d?",id:"q-is-any-red-hat-subscription-required-to-use-llm-d",level:2},{value:"Q: We don\u2019t use podman. Can we still use llm-d?.",id:"q-we-dont-use-podman-can-we-still-use-llm-d",level:2},{value:"Q: We don\u2019t use vLLM. Can we still try llm-d?",id:"q-we-dont-use-vllm-can-we-still-try-llm-d",level:2},{value:"<strong>A:</strong> While llm-d is deeply integrated with vLLM, if the community wanted to add support for another inference engine, the community and maintainers would review it as any proposal..",id:"a-while-llm-d-is-deeply-integrated-with-vllm-if-the-community-wanted-to-add-support-for-another-inference-engine-the-community-and-maintainers-would-review-it-as-any-proposal",level:2},{value:"Q: What is an AI-aware inference-optimized router?",id:"q-what-is-an-ai-aware-inference-optimized-router",level:2},{value:"Q: What is a scorer?",id:"q-what-is-a-scorer",level:2},{value:"Q: What is KV Cache Aware Routing in llm-d?",id:"q-what-is-kv-cache-aware-routing-in-llm-d",level:2},{value:"Q: What is prefill/decode disaggregation in llm-d?",id:"q-what-is-prefilldecode-disaggregation-in-llm-d",level:2},{value:"Q: What is LMCache&#39;s role in llm-d?",id:"q-what-is-lmcaches-role-in-llm-d",level:2},{value:"Q: Where do I submit feedback, bug reports or feature requests?",id:"q-where-do-i-submit-feedback-bug-reports-or-feature-requests",level:2}];function c(e){const t={h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"frequently-asked-questions",children:"Frequently Asked Questions"})}),"\n",(0,r.jsx)(t.h2,{id:"q-what-is-llm-d",children:"Q: What is llm-d?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," llm-d is a new open source project focused on providing distributed inferencing for Generative AI runtimes on any Kubernetes cluster. Its architecture is designed for high performance and scalability, aiming to reduce costs through a spectrum of hardware and software efficiency improvements. llm-d prioritizes ease of deployment and use, as well as the operational needs of running large GPU clusters, including SRE concerns and day 2 operations. At launch, its key features include prefill/decode disaggregation, KV cache distribution and management, an AI-aware router with customizable scoring, operational telemetry, Kubernetes-based deployment, and the NIXL inference optimized transfer library."]}),"\n",(0,r.jsx)(t.h2,{id:"q-what-is-the-goal-of-the-llm-d-project",children:"Q: What is the goal of the llm-d project?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," The llm-d project aims to efficiently route and manage AI inference requests, specifically leveraging cache-aware caching algorithms to improve inference performance. The overall goal is to develop a flexible, efficient routing mechanism based on prefix caching or advanced cache awareness."]}),"\n",(0,r.jsx)(t.h2,{id:"q-what-is-the-engineering-design-philosophy-for-the-project",children:"Q: What is the engineering design philosophy for the project?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"}),"\r\nllm-d will leverage existing OSS projects where possible.\r\nWhile the initial MVP release supports NVIDIA GPUs (including Blackwell), we welcome support to be added for any accelerator. For example, we have significant work underway with AMD.and Intel.\r\nLanguage-model focused for MVP, adding multi-modal support as prioritized by community.\r\nAll work on vLLM itself must happen upstream in the vllm-project to ensure it remains the reference implementation within the vLLM community.\r\nWe will build a system that autonomously meets user cost/performance targets while maintaining user-defined SLOs.\r\nAspiring to vLLM itself, we will have the most performant and featureful open source Intelligent router for AI workloads on Kubernetes."]}),"\n",(0,r.jsx)(t.h2,{id:"q-what-are-the-key-features-of-llm-d-being-presented-at-red-hat-summit",children:"Q: What are the key features of llm-d being presented at Red Hat Summit?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," At Red Hat Summit, we plan to showcase three key features: prefill/decode disaggregation, KV Cache aware routing, and an AI-aware inference-optimized router. These demos will highlight the performance benefits of these features, demonstrating improvements in Time-To-First-Token (TTFT), reduced latency, increased throughput, and improve resource utilization, leading to significant cost savings. Our goal is to show that both user experience (TTFT, latency, throughput) and cost-efficiency can be achieved simultaneously through a continuous stream of technical innovation from the llm-d community.."]}),"\n",(0,r.jsx)(t.h2,{id:"q-is-any-red-hat-subscription-required-to-use-llm-d",children:"Q: Is any Red Hat subscription required to use llm-d?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," No. In fact we worked with CoreWeave to add support for CoreWeave Kubernetes Service (ubuntu) for launch!"]}),"\n",(0,r.jsx)(t.h2,{id:"q-we-dont-use-podman-can-we-still-use-llm-d",children:"Q: We don\u2019t use podman. Can we still use llm-d?."}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," Any CRI compatible container engine (such as docker-ce) will work, and we document it."]}),"\n",(0,r.jsx)(t.h2,{id:"q-we-dont-use-vllm-can-we-still-try-llm-d",children:"Q: We don\u2019t use vLLM. Can we still try llm-d?"}),"\n",(0,r.jsxs)(t.h2,{id:"a-while-llm-d-is-deeply-integrated-with-vllm-if-the-community-wanted-to-add-support-for-another-inference-engine-the-community-and-maintainers-would-review-it-as-any-proposal",children:[(0,r.jsx)(t.strong,{children:"A:"})," While llm-d is deeply integrated with vLLM, if the community wanted to add support for another inference engine, the community and maintainers would review it as any proposal.."]}),"\n",(0,r.jsx)(t.h2,{id:"q-what-is-an-ai-aware-inference-optimized-router",children:"Q: What is an AI-aware inference-optimized router?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," The router implementation in llm-d is one of its key features. We leverage the Gateway API Inference Extension for Kubernetes. As founding members of wg-serving and the inference extensions workstream, the group of launch partners behind llm-d have extended GAIE significantly during the bootstrapping of the llm-d project.. Our default implementation uses Envoy Proxy. The router will be the primary UX for a spectrum of software efficiency improvements (implemented as scheduler plugins) called scorers."]}),"\n",(0,r.jsx)(t.h2,{id:"q-what-is-a-scorer",children:"Q: What is a scorer?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"}),' Along with llm-d, we have also released a set of custom plugins that implement features like prefill/decode disaggregation. Other scorers optimize performance based on prefix, load or kv-cache locality. These scheduler components get loaded into the Gateway component of llm-d. An "Endpoint Picker (EPP)" binary and container images are provided which can be configured via Envoy\'s ext-proc feature to make optimized routing decisions for AI Inference requests to backend model serving platforms (e.g. vLLM).']}),"\n",(0,r.jsxs)(t.p,{children:["This functionality is built upon Gateway API and the Gateway API Inference Extension (GIE) projects for both the API resources and machinery, but extends support beyond what's available in those projects by loading other custom plugins needed by llm-d (e.g. custom scorers, P/D Disaggregation, etc).\r\n",(0,r.jsx)(t.strong,{children:"A:"})," It looks at a prompt and decides what to do with it based on user desired state."]}),"\n",(0,r.jsx)(t.p,{children:"Because we know what type of traffic we will handle (we see L7), we can optimize the router\u2019s behavior for inference."}),"\n",(0,r.jsx)(t.h2,{id:"q-what-is-kv-cache-aware-routing-in-llm-d",children:"Q: What is KV Cache Aware Routing in llm-d?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," KV Cache Aware Routing in llm-d is a mechanism that optimizes the routing of inference requests based on the availability and reuse of the KV cache. The KV cache stores previously computed key and value attention vectors, which can significantly reduce computation time for subsequent requests, especially those sharing similar prefixes or context. By routing requests to nodes with existing relevant KV cache, llm-d aims to reduce Time-To-First-Token (TTFT) and improve overall performance. This routing is facilitated by a global KV cache index that maps cache blocks to their locations, allowing the router to make informed decisions about where to send each request. LMCache is involved as a KV cache engine."]}),"\n",(0,r.jsx)(t.h2,{id:"q-what-is-prefilldecode-disaggregation-in-llm-d",children:"Q: What is prefill/decode disaggregation in llm-d?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," Prefill/decode disaggregation is a feature in llm-d that separates the inference process into two stages: prefill and decode. The prefill stage handles the initial processing of the input prompt, including tokenization and KV cache population. The decode stage generates the actual output tokens using the prefilled cache. This separation allows for optimized allocation of compute resources, as different types of hardware or configurations might be better suited for each stage. It aims to improve resource utilization, enable more efficient scheduling, and maintain high throughput and low latency, especially in heterogeneous environments. NIXL is used for KV transport in this process."]}),"\n",(0,r.jsx)(t.h2,{id:"q-what-is-lmcaches-role-in-llm-d",children:"Q: What is LMCache's role in llm-d?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," LMCache acts as a KV Cache Engine within llm-d. Its role is to support KV cache offloading and sharing between vLLM nodes, which enhances performance by enabling reuse of previously computed KV caches. LMCache maintains a shared KV cache metadata store (Redis) that is used for KV Cache Indexing, allowing the system to track the location of cache blocks and optimize routing for KV cache hits. LMCache is currently the only open-source KVCache offloading/reusing engine and is crucial for enabling features like KV Cache Aware Routing in llm-d. It facilitates cross-node sharing by maintaining a global KV locality-index, enabling cross-node KVCache reuse. We spent time making these interfaces clean and pluggable."]}),"\n",(0,r.jsx)(t.h2,{id:"q-where-do-i-submit-feedback-bug-reports-or-feature-requests",children:"Q: Where do I submit feedback, bug reports or feature requests?"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.strong,{children:"A:"})," Check out the CONTRIBUTING.md. We\u2019re using GitHub, so submit your issue or PR to the relevant repository."]})]})}function h(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>s,x:()=>o});var n=i(6540);const r={},a=n.createContext(r);function s(e){const t=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),n.createElement(a.Provider,{value:t},e.children)}}}]);