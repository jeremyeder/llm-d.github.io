"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[8482],{8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var t=r(6540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},8732:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"architecture/architecture","title":"Overview of llm-d architecture","description":"What is llm-d?","source":"@site/docs/architecture/00_architecture.md","sourceDirName":"architecture","slug":"/architecture/architecture","permalink":"/webdocs/docs/architecture/architecture","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"sidebar_position":0,"label":"llm-d Architecture"},"sidebar":"structureSidebar","next":{"title":"Frequently Asked Questions","permalink":"/webdocs/docs/architecture/faq"}}');var i=r(4848),s=r(8453);const o={sidebar_position:0,label:"llm-d Architecture"},a="Overview of llm-d architecture",l={},c=[{value:"Architecture",id:"architecture",level:3}];function d(e){const n={h1:"h1",h3:"h3",header:"header",img:"img",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"overview-of-llm-d-architecture",children:"Overview of llm-d architecture"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What is llm-d?"})}),"\n",(0,i.jsx)(n.p,{children:"llm-d is an open source project providing distributed inferencing for GenAI runtimes on any Kubernetes cluster. Its highly performant, scalable architecture helps reduce costs through a spectrum of hardware efficiency improvements. The project prioritizes ease of deployment+use as well as SRE needs + day 2 operations associated with running large GPU clusters."}),"\n",(0,i.jsx)(n.p,{children:"It includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Prefill/decode disaggregation"}),"\n",(0,i.jsx)(n.li,{children:"KV Cache distribution, offloading and storage hierarchy"}),"\n",(0,i.jsx)(n.li,{children:"AI-aware router with plug points for customizable scorers"}),"\n",(0,i.jsx)(n.li,{children:"Operational telemetry for production, prometheus/grafana"}),"\n",(0,i.jsx)(n.li,{children:"Kubernetes-based, works on OCP, minikube, and other k8s distributions"}),"\n",(0,i.jsx)(n.li,{children:"NIXL inference transfer library"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"llm-d consists of the following components:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Gateway API Inference Extension (GIE) - This extension upgrades an ext-proc-capable proxy or gateway - such as Envoy Gateway, kGateway, or the GKE Gateway - to become an inference gateway - supporting inference platform teams self-hosting large language models on Kubernetes. This integration makes it easy to expose and control access to your local OpenAI-compatible chat completion endpoints to other workloads on or off cluster, or to integrate your self-hosted models alongside model-as-a-service providers in a higher level AI Gateway like LiteLLM, Solo AI Gateway, or Apigee.\r\nThe inference gateway:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improves the tail latency and throughput of LLM completion requests against Kubernetes-hosted model servers using an extendable request scheduling algorithm that is kv-cache and request cost aware, avoiding evictions or queueing as load increases"}),"\n",(0,i.jsx)(n.li,{children:"Provides Kubernetes-native declarative APIs to route client model names to use-case specific LoRA adapters and control incremental rollout of new adapter versions, A/B traffic splitting, and safe blue-green base model and model server upgrades"}),"\n",(0,i.jsx)(n.li,{children:"Adds end to end observability around service objective attainment"}),"\n",(0,i.jsx)(n.li,{children:"Ensures operational guardrails between different client model names, allowing a platform team to safely serve many different GenAI workloads on the same pool of shared foundation model servers for higher utilization and fewer required accelerators"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Distributed KV Cache"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"LMCache (in llm-d container)"}),"\n",(0,i.jsx)(n.li,{children:"NIXL (in llm-d container)"}),"\n",(0,i.jsx)(n.li,{children:"KVCache Indexer"}),"\n",(0,i.jsx)(n.li,{children:"Redis"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Model Service Controller - ModelService is a Kubernetes operator (CRD + controller) that enables the creation of vllm pods and routing resources for a given model."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Enables disaggregated prefill"}),"\n",(0,i.jsx)(n.li,{children:"Supports creation of Gateway API Inference Extension resources for routing"}),"\n",(0,i.jsx)(n.li,{children:"Supports auto-scaling with HPA"}),"\n",(0,i.jsx)(n.li,{children:"Supports independent scaling of prefill and decode instances"}),"\n",(0,i.jsx)(n.li,{children:"Supports independent node affinities for prefill and decode instances"}),"\n",(0,i.jsx)(n.li,{children:"Supports model loading from OCI images, HuggingFace public and private registries, and PVCs"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Metrics Service (Prometheus)"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"llm-d Architecture",src:r(8771).A+"",width:"4294",height:"3725"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8771:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/arch-f25a9e4f2067f2a7959c29b0ad35999e.jpg"}}]);