"use strict";(self.webpackChunkdocusaurus_test=self.webpackChunkdocusaurus_test||[]).push([[594],{7829:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"guide/Installation/quickstart","title":"Trying llm-d via the Quick Start installer","description":"Getting Started with llm-d on Kubernetes.  For specific instructions on how to install llm-d on minikube, see the README-minikube.md instructions.","source":"@site/docs/guide/Installation/quickstart.md","sourceDirName":"guide/Installation","slug":"/guide/Installation/quickstart","permalink":"/docs/guide/Installation/quickstart","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Quick Start installer"},"sidebar":"guideSidebar","previous":{"title":"Prerequisites","permalink":"/docs/guide/Installation/prerequisites"}}');var i=l(4848),r=l(8453);const t={sidebar_position:2,sidebar_label:"Quick Start installer"},a="Trying llm-d via the Quick Start installer",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"llm-d Installation",id:"llm-d-installation",level:2},{value:"Usage",id:"usage",level:3},{value:"Flags",id:"flags",level:3},{value:"Examples",id:"examples",level:2},{value:"Install llm-d on an Existing Kubernetes Cluster",id:"install-llm-d-on-an-existing-kubernetes-cluster",level:3},{value:"Install on OpenShift",id:"install-on-openshift",level:3},{value:"Validation",id:"validation",level:3},{value:"Customizing your deployment",id:"customizing-your-deployment",level:3},{value:"Sample Application and Model Configuration",id:"sample-application-and-model-configuration",level:4},{value:"Feature Flags",id:"feature-flags",level:4},{value:"Metrics Collection",id:"metrics-collection",level:3},{value:"Accessing the Metrics UIs",id:"accessing-the-metrics-uis",level:4},{value:"Option 1: Port Forwarding (Default)",id:"option-1-port-forwarding-default",level:5},{value:"Option 2: Ingress (Optional)",id:"option-2-ingress-optional",level:5},{value:"Option 3: OpenShift",id:"option-3-openshift",level:5},{value:"Available Metrics",id:"available-metrics",level:4},{value:"Security Note",id:"security-note",level:4},{value:"Troubleshooting",id:"troubleshooting",level:3},{value:"Uninstall",id:"uninstall",level:3}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"trying-llm-d-via-the-quick-start-installer",children:"Trying llm-d via the Quick Start installer"})}),"\n",(0,i.jsxs)(n.p,{children:["Getting Started with llm-d on Kubernetes.  For specific instructions on how to install llm-d on minikube, see the ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-deployer/blob/main/quickstart/README-minikube.md",children:"README-minikube.md"})," instructions."]}),"\n",(0,i.jsxs)(n.p,{children:["For more information on llm-d in general, see the llm-d git repository ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d",children:"here"})," and website ",(0,i.jsx)(n.a,{href:"https://llm-d.ai",children:"here"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This guide will walk you through the steps to install and deploy llm-d on a Kubernetes cluster, using an opinionated flow in order to get up and running as quickly as possible."}),"\n",(0,i.jsxs)(n.p,{children:["For more information on llm-d, see the llm-d git repository ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d",children:"here"})," and website ",(0,i.jsx)(n.a,{href:"https://llmd.io",children:"here"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.p,{children:["First ensure you have all the tools and resources as described in ",(0,i.jsx)(n.a,{href:"/docs/guide/Installation/prerequisites",children:"Prerequisites"})]}),"\n",(0,i.jsx)("a",{name:"install"}),"\n",(0,i.jsx)(n.h2,{id:"llm-d-installation",children:"llm-d Installation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Change to the directory holding your clone of the llm-d-deployer code"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Navigate to the quickstart directory, e.g."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd llm-d-deployer/quickstart\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Only a single installation of llm-d on a cluster is currently supported.  In the future, multiple model services will be supported.  Until then, ",(0,i.jsx)(n.a,{href:"#uninstall",children:"uninstall llm-d"})," before reinstalling."]}),"\n",(0,i.jsxs)(n.p,{children:["The llm-d-deployer contains all the helm charts necessary to deploy llm-d. To facilitate the installation of the helm charts, the ",(0,i.jsx)(n.code,{children:"llmd-installer.sh"})," script is provided. This script will populate the necessary manifests in the ",(0,i.jsx)(n.code,{children:"manifests"})," directory.\r\nAfter this, it will apply all the manifests in order to bring up the cluster."]}),"\n",(0,i.jsx)(n.p,{children:"The llmd-installer.sh script aims to simplify the installation of llm-d using the llm-d-deployer as it's main function.  It scripts as many of the steps as possible to make the installation process more streamlined.  This includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Installing the GAIE infrastructure"}),"\n",(0,i.jsx)(n.li,{children:"Creating the namespace with any special configurations"}),"\n",(0,i.jsx)(n.li,{children:"Creating the pull secret to download the images"}),"\n",(0,i.jsx)(n.li,{children:"Creating the model service CRDs"}),"\n",(0,i.jsx)(n.li,{children:"Applying the helm charts"}),"\n",(0,i.jsx)(n.li,{children:"Deploying the sample app (model service)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"It also supports uninstalling the llm-d infrastructure and the sample app."}),"\n",(0,i.jsxs)(n.p,{children:["Before proceeding with the installation, ensure you have completed the prerequisites and are able to issue ",(0,i.jsx)(n.code,{children:"kubectl"})," or ",(0,i.jsx)(n.code,{children:"oc"})," commands to your cluster by configuring your ",(0,i.jsx)(n.code,{children:"~/.kube/config"})," file or by using the ",(0,i.jsx)(n.code,{children:"oc login"})," command."]}),"\n",(0,i.jsx)(n.h3,{id:"usage",children:"Usage"}),"\n",(0,i.jsxs)(n.p,{children:["The installer needs to be run from the ",(0,i.jsx)(n.code,{children:"llm-d-deployer/quickstart"})," directory as a cluster admin with CLI access to the cluster."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./llmd-installer.sh [OPTIONS]\n"})}),"\n",(0,i.jsx)(n.h3,{id:"flags",children:"Flags"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Flag"}),(0,i.jsx)(n.th,{children:"Description"}),(0,i.jsx)(n.th,{children:"Example"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-a"}),", ",(0,i.jsx)(n.code,{children:"--auth-file PATH"})]}),(0,i.jsx)(n.td,{children:"Path to containers auth.json"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --auth-file ~/.config/containers/auth.json"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-z"}),", ",(0,i.jsx)(n.code,{children:"--storage-size SIZE"})]}),(0,i.jsx)(n.td,{children:"Size of storage volume"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --storage-size 15Gi"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-c"}),", ",(0,i.jsx)(n.code,{children:"--storage-class CLASS"})]}),(0,i.jsx)(n.td,{children:"Storage class to use (default: efs-sc)"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --storage-class ocs-storagecluster-cephfs"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-n"}),", ",(0,i.jsx)(n.code,{children:"--namespace NAME"})]}),(0,i.jsx)(n.td,{children:"K8s namespace (default: llm-d)"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --namespace foo"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-f"}),", ",(0,i.jsx)(n.code,{children:"--values-file PATH"})]}),(0,i.jsx)(n.td,{children:"Path to Helm values.yaml file (default: values.yaml)"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --values-file /path/to/values.yaml"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-u"}),", ",(0,i.jsx)(n.code,{children:"--uninstall"})]}),(0,i.jsx)(n.td,{children:"Uninstall the llm-d components from the current cluster"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --uninstall"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-d"}),", ",(0,i.jsx)(n.code,{children:"--debug"})]}),(0,i.jsx)(n.td,{children:"Add debug mode to the helm install"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --debug"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-i"}),", ",(0,i.jsx)(n.code,{children:"--skip-infra"})]}),(0,i.jsx)(n.td,{children:"Skip the infrastructure components of the installation"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --skip-infra"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-t"}),", ",(0,i.jsx)(n.code,{children:"--download-timeout"})]}),(0,i.jsx)(n.td,{children:"Timeout for model download job"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --download-timeout"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-D"}),", ",(0,i.jsx)(n.code,{children:"--download-model"})]}),(0,i.jsx)(n.td,{children:"Download the model to PVC from Hugging Face"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --download-model"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-m"}),", ",(0,i.jsx)(n.code,{children:"--disable-metrics-collection"})]}),(0,i.jsx)(n.td,{children:"Disable metrics collection (Prometheus will not be installed)"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --disable-metrics-collection"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"-h"}),", ",(0,i.jsx)(n.code,{children:"--help"})]}),(0,i.jsx)(n.td,{children:"Show this help and exit"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"./llmd-installer.sh --help"})})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,i.jsx)(n.h3,{id:"install-llm-d-on-an-existing-kubernetes-cluster",children:"Install llm-d on an Existing Kubernetes Cluster"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export HF_TOKEN="your-token"\r\n./llmd-installer.sh\n'})}),"\n",(0,i.jsx)(n.h3,{id:"install-on-openshift",children:"Install on OpenShift"}),"\n",(0,i.jsx)(n.p,{children:"Before running the installer, ensure you have logged into the cluster as a cluster administrator.  For example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"oc login --token=sha256~yourtoken --server=https://api.yourcluster.com:6443\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export HF_TOKEN="your-token"\r\n./llmd-installer.sh\n'})}),"\n",(0,i.jsx)(n.h3,{id:"validation",children:"Validation"}),"\n",(0,i.jsxs)(n.p,{children:["The inference-gateway serves as the HTTP ingress point for all inference requests in our deployment.\r\nIt\u2019s implemented as a Kubernetes Gateway (",(0,i.jsx)(n.code,{children:"gateway.networking.k8s.io/v1"}),") using either kgateway or istio as the\r\ngatewayClassName, and sits in front of your inference pods to handle path-based routing, load balancing, retries,\r\nand metrics. This example validates that the gateway itself is routing your completion requests correctly.\r\nYou can execute the ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-deployer/blob/main/quickstart/test-request.sh",children:(0,i.jsx)(n.code,{children:"test-request.sh"})})," script in the quickstart folder to test on the cluster."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:'If you receive an error indicating PodSecurity "restricted" violations when running the smoke-test script, you\r\nneed to remove the restrictive PodSecurity labels from the namespace. Once these labels are removed, re-run the\r\nscript and it should proceed without PodSecurity errors.\r\nRun the following command:'}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl label namespace <NAMESPACE> \\\r\n  pod-security.kubernetes.io/warn- \\\r\n  pod-security.kubernetes.io/warn-version- \\\r\n  pod-security.kubernetes.io/audit- \\\r\n  pod-security.kubernetes.io/audit-version-\n"})}),"\n",(0,i.jsx)(n.h3,{id:"customizing-your-deployment",children:"Customizing your deployment"}),"\n",(0,i.jsxs)(n.p,{children:["The helm charts can be customized by modifying the ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-deployer/blob/main/charts/llm-d/values.yaml",children:"values.yaml"})," file.  However, it is recommended to override values in the ",(0,i.jsx)(n.code,{children:"values.yaml"})," by creating a custom yaml file and passing it to the installer using the ",(0,i.jsx)(n.code,{children:"--values-file"})," flag.\r\nSeveral examples are provided in the ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-deployer/blob/main/quickstart/examples",children:"examples"})," directory.  You would invoke the installer with the following command:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./llmd-installer.sh --values-file ./examples/base.yaml\n"})}),"\n",(0,i.jsxs)(n.p,{children:["These files are designed to be used as a starting point to customize your deployment.  Refer to the ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-deployer/blob/main/charts/llm-d/values.yaml",children:"values.yaml"})," file for all the possible options."]}),"\n",(0,i.jsx)(n.h4,{id:"sample-application-and-model-configuration",children:"Sample Application and Model Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Some of the more common options for changing the sample application model are:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sampleApplication.model.modelArtifactURI"})," - The URI of the model to use.  This is the path to the model either to Hugging Face (",(0,i.jsx)(n.code,{children:"hf://meta-llama/Llama-3.2-3B-Instruct"}),") or a persistent volume claim (PVC) (",(0,i.jsx)(n.code,{children:"pvc://model-pvc/meta-llama/Llama-3.2-1B-Instruct"}),").  Using a PVC can be paired with the ",(0,i.jsx)(n.code,{children:"--download-model"})," flag to download the model to PVC."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sampleApplication.model.modelName"})," - The name of the model to use.  This will be used in the naming of deployed resources and also the model ID when using the API."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sampleApplication.baseConfigMapRefName"})," - The name of the preset base configuration to use.  This will depend on the features you want to enable."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sampleApplication.prefill.replicas"})," - The number of prefill replicas to deploy."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sampleApplication.decode.replicas"})," - The number of decode replicas to deploy."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'sampleApplication:\r\n  model:\r\n    modelArtifactURI: hf://meta-llama/Llama-3.2-1B-Instruct\r\n    modelName: "llama3-1B"\r\n  baseConfigMapRefName: basic-gpu-with-nixl-and-redis-lookup-preset\r\n  prefill:\r\n    replicas: 1\r\n  decode:\r\n    replicas: 1\n'})}),"\n",(0,i.jsx)(n.h4,{id:"feature-flags",children:"Feature Flags"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"redis.enabled"})," - Whether to enable Redis needed to enable the KV Cache Aware Scorer\r\n",(0,i.jsx)(n.code,{children:"modelservice.epp.defaultEnvVarsOverride"})," - The environment variables to override for the model service.  For each feature flag, you can set the value to ",(0,i.jsx)(n.code,{children:"true"})," or ",(0,i.jsx)(n.code,{children:"false"})," to enable or disable the feature."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'redis:\r\n  enabled: true\r\nmodelservice:\r\n  epp:\r\n    defaultEnvVarsOverride:\r\n      - name: ENABLE_KVCACHE_AWARE_SCORER\r\n        value: "false"\r\n      - name: ENABLE_PREFIX_AWARE_SCORER\r\n        value: "true"\r\n      - name: ENABLE_LOAD_AWARE_SCORER\r\n        value: "true"\r\n      - name: ENABLE_SESSION_AWARE_SCORER\r\n        value: "false"\r\n      - name: PD_ENABLED\r\n        value: "false"\r\n      - name: PD_PROMPT_LEN_THRESHOLD\r\n        value: "10"\r\n      - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER\r\n        value: "false"\r\n      - name: PREFILL_ENABLE_LOAD_AWARE_SCORER\r\n        value: "false"\r\n      - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER\r\n        value: "false"\r\n      - name: PREFILL_ENABLE_SESSION_AWARE_SCORER\r\n        value: "false"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"metrics-collection",children:"Metrics Collection"}),"\n",(0,i.jsxs)(n.p,{children:["llm-d includes built-in support for metrics collection using Prometheus and Grafana. This feature is enabled by default but can be disabled using the\r\n",(0,i.jsx)(n.code,{children:"--disable-metrics-collection"})," flag during installation. In OpenShift, llm-d applies ServiceMonitors for llm-d components that trigger Prometheus\r\nscrape targets for the built-in user workload monitoring Prometheus stack."]}),"\n",(0,i.jsx)(n.h4,{id:"accessing-the-metrics-uis",children:"Accessing the Metrics UIs"}),"\n",(0,i.jsxs)(n.p,{children:["If running in OpenShift, skip to ",(0,i.jsx)(n.a,{href:"#option-3-openshift",children:"Option 3: OpenShift"}),"."]}),"\n",(0,i.jsx)(n.h5,{id:"option-1-port-forwarding-default",children:"Option 1: Port Forwarding (Default)"}),"\n",(0,i.jsx)(n.p,{children:"Once installed, you can access the metrics UIs through port-forwarding:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Prometheus UI (port 9090):"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward -n llm-d-monitoring --address 0.0.0.0 svc/prometheus-kube-prometheus-prometheus 9090:9090\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Grafana UI (port 3000):"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward -n llm-d-monitoring --address 0.0.0.0 svc/prometheus-grafana 3000:80\n"})}),"\n",(0,i.jsx)(n.p,{children:"Access the UIs at:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Prometheus: ",(0,i.jsx)(n.a,{href:"#",children:"http://YOUR_IP:9090"})]}),"\n",(0,i.jsxs)(n.li,{children:["Grafana: ",(0,i.jsx)(n.a,{href:"#",children:"http://YOUR_IP:3000"})," (default credentials: admin/admin)"]}),"\n"]}),"\n",(0,i.jsx)(n.h5,{id:"option-2-ingress-optional",children:"Option 2: Ingress (Optional)"}),"\n",(0,i.jsx)(n.p,{children:"For production environments, you can configure ingress for both Prometheus and Grafana. Add the following to your values.yaml:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"prometheus:\r\n  ingress:\r\n    enabled: true\r\n    annotations:\r\n      kubernetes.io/ingress.class: nginx\r\n    hosts:\r\n      - prometheus.your-domain.com\r\n    tls:\r\n      - secretName: prometheus-tls\r\n        hosts:\r\n          - prometheus.your-domain.com\r\n\r\ngrafana:\r\n  ingress:\r\n    enabled: true\r\n    annotations:\r\n      kubernetes.io/ingress.class: nginx\r\n    hosts:\r\n      - grafana.your-domain.com\r\n    tls:\r\n      - secretName: grafana-tls\r\n        hosts:\r\n          - grafana.your-domain.com\n"})}),"\n",(0,i.jsx)(n.h5,{id:"option-3-openshift",children:"Option 3: OpenShift"}),"\n",(0,i.jsx)(n.p,{children:"If you're using OpenShift with user workload monitoring enabled, you can access the metrics through the OpenShift console:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Navigate to the OpenShift console"}),"\n",(0,i.jsx)(n.li,{children:'In the left navigation bar, click on "Observe"'}),"\n",(0,i.jsxs)(n.li,{children:["You can access:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Metrics: Click on "Metrics" to view and query metrics using the built-in Prometheus UI'}),"\n",(0,i.jsx)(n.li,{children:'Targets: Click on "Targets" to see all monitored endpoints and their status'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The metrics are automatically integrated into the OpenShift monitoring stack, providing a seamless experience for viewing and analyzing your llm-d metrics.\r\nThe llm-d-deployer does not install Grafana in OpenShift, but it's recommended that users install Grafana to view metrics and import dashboards."}),"\n",(0,i.jsxs)(n.p,{children:["Follow the ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d/blob/dev/observability/openshift/README.md#step-2-set-up-grafana-optional",children:"OpenShift Grafana setup guide"}),"\r\nThe guide includes manifests to install the following:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Grafana instance"}),"\n",(0,i.jsx)(n.li,{children:"Grafana Prometheus datasource from user workload monitoring stack"}),"\n",(0,i.jsx)(n.li,{children:"Grafana llm-d dashboard"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"available-metrics",children:"Available Metrics"}),"\n",(0,i.jsx)(n.p,{children:"The metrics collection includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Model inference performance metrics"}),"\n",(0,i.jsx)(n.li,{children:"Request latency and throughput"}),"\n",(0,i.jsx)(n.li,{children:"Resource utilization (CPU, memory, GPU)"}),"\n",(0,i.jsx)(n.li,{children:"Cache hit/miss rates"}),"\n",(0,i.jsx)(n.li,{children:"Error rates and types"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"security-note",children:"Security Note"}),"\n",(0,i.jsx)(n.p,{children:"When running in a cloud environment (like EC2), make sure to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Configure your security groups to allow inbound traffic on ports 9090 and 3000 (if using port-forwarding)"}),"\n",(0,i.jsxs)(n.li,{children:["Use the ",(0,i.jsx)(n.code,{children:"--address 0.0.0.0"})," flag with port-forward to allow external access"]}),"\n",(0,i.jsx)(n.li,{children:"Consider setting up proper authentication for production environments"}),"\n",(0,i.jsx)(n.li,{children:"If using ingress, ensure proper TLS configuration and authentication"}),"\n",(0,i.jsx)(n.li,{children:"For OpenShift, consider using the built-in OAuth integration for Grafana"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.p,{children:"The various images can take some time to download depending on your connectivity. Watching events\r\nand logs of the prefill and decode pods is a good place to start. Here are some examples to help\r\nyou get started."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# View the status of the pods in the default llm-d namespace. Replace "llm-d" if you used a custom namespace on install\r\nkubectl get pods -n llm-d\r\n\r\n# Describe all prefill pods:\r\nkubectl describe pods -l llm-d.ai/role=prefill -n llm-d\r\n\r\n# Fetch logs from each prefill pod:\r\nkubectl logs -l llm-d.ai/role=prefill --all-containers=true -n llm-d --tail=200\r\n\r\n# Describe all decode pods:\r\nkubectl describe pods -l llm-d.ai/role=decode -n llm-d\r\n\r\n# Fetch logs from each decode pod:\r\nkubectl logs -l llm-d.ai/role=decode --all-containers=true -n llm-d --tail=200\r\n\r\n# Describe all endpoint-picker pods:\r\nkubectl describe pod -n llm-d -l llm-d.ai/epp\r\n\r\n# Fetch logs from each endpoint-picker pod:\r\nkubectl logs -n llm-d -l llm-d.ai/epp --all-containers=true --tail=200\n'})}),"\n",(0,i.jsxs)(n.p,{children:["More examples of debugging logs can be found ",(0,i.jsx)(n.a,{href:"https://github.com/llm-d/llm-d-deployer/blob/main/quickstart/examples/no-features/README.md",children:"here"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"uninstall",children:"Uninstall"}),"\n",(0,i.jsx)(n.p,{children:"This will remove llm-d resources from the cluster. This is useful, especially for test/dev if you want to\r\nmake a change, simply uninstall and then run the installer again with any changes you make."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./llmd-installer.sh --uninstall\n"})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,l)=>{l.d(n,{R:()=>t,x:()=>a});var s=l(6540);const i={},r=s.createContext(i);function t(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);