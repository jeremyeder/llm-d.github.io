<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-architecture/faq" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Frequently Asked Questions | llm-d Website</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://llm-d.github.io/llm-d-website/docs/architecture/faq"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Frequently Asked Questions | llm-d Website"><meta data-rh="true" name="description" content="Q: What is llm-d?"><meta data-rh="true" property="og:description" content="Q: What is llm-d?"><link data-rh="true" rel="icon" href="/llm-d-website/img/llm-d-favicon.png"><link data-rh="true" rel="canonical" href="https://llm-d.github.io/llm-d-website/docs/architecture/faq"><link data-rh="true" rel="alternate" href="https://llm-d.github.io/llm-d-website/docs/architecture/faq" hreflang="en"><link data-rh="true" rel="alternate" href="https://llm-d.github.io/llm-d-website/docs/architecture/faq" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/llm-d-website/blog/rss.xml" title="llm-d Website RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/llm-d-website/blog/atom.xml" title="llm-d Website Atom Feed"><link rel="stylesheet" href="/llm-d-website/assets/css/styles.503a682c.css">
<script src="/llm-d-website/assets/js/runtime~main.3d0dc4c3.js" defer="defer"></script>
<script src="/llm-d-website/assets/js/main.235a6b24.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/llm-d-website/img/llm-d-icon.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/llm-d-website/"><div class="navbar__logo"><img src="/llm-d-website/img/llm-d-icon.png" alt="llm-d Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/llm-d-website/img/llm-d-icon.png" alt="llm-d Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/llm-d-website/docs/guide">User Guide</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/llm-d-website/docs/architecture/architecture">What is llm-d?</a><a class="navbar__item navbar__link" href="/llm-d-website/docs/community/work_with_us">Community</a><a class="navbar__item navbar__link" href="/llm-d-website/blog">News</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/llm-d/llm-d-website.github.io/tree/main/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/llm-d-website/docs/architecture/architecture">Overview of llm-d architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/llm-d-website/docs/architecture/faq">Frequently Asked Questions</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/llm-d-website/docs/architecture/Components/deployer">Components</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/llm-d-website/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Frequently Asked Questions</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Frequently Asked Questions</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-what-is-llm-d">Q: What is llm-d?<a href="#q-what-is-llm-d" class="hash-link" aria-label="Direct link to Q: What is llm-d?" title="Direct link to Q: What is llm-d?">â€‹</a></h2>
<p><strong>A:</strong> llm-d is a new open source project focused on providing distributed inferencing for Generative AI runtimes on any Kubernetes cluster. Its architecture is designed for high performance and scalability, aiming to reduce costs through a spectrum of hardware and software efficiency improvements. llm-d prioritizes ease of deployment and use, as well as the operational needs of running large GPU clusters, including SRE concerns and day 2 operations. At launch, its key features include prefill/decode disaggregation, KV cache distribution and management, an AI-aware router with customizable scoring, operational telemetry, Kubernetes-based deployment, and the NIXL inference optimized transfer library.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-what-is-the-goal-of-the-llm-d-project">Q: What is the goal of the llm-d project?<a href="#q-what-is-the-goal-of-the-llm-d-project" class="hash-link" aria-label="Direct link to Q: What is the goal of the llm-d project?" title="Direct link to Q: What is the goal of the llm-d project?">â€‹</a></h2>
<p><strong>A:</strong> The llm-d project aims to efficiently route and manage AI inference requests, specifically leveraging cache-aware caching algorithms to improve inference performance. The overall goal is to develop a flexible, efficient routing mechanism based on prefix caching or advanced cache awareness.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-what-is-the-engineering-design-philosophy-for-the-project">Q: What is the engineering design philosophy for the project?<a href="#q-what-is-the-engineering-design-philosophy-for-the-project" class="hash-link" aria-label="Direct link to Q: What is the engineering design philosophy for the project?" title="Direct link to Q: What is the engineering design philosophy for the project?">â€‹</a></h2>
<p><strong>A:</strong>
llm-d will leverage existing OSS projects where possible.
While the initial MVP release supports NVIDIA GPUs (including Blackwell), we welcome support to be added for any accelerator. For example, we have significant work underway with AMD.and Intel.
Language-model focused for MVP, adding multi-modal support as prioritized by community.
All work on vLLM itself must happen upstream in the vllm-project to ensure it remains the reference implementation within the vLLM community.
We will build a system that autonomously meets user cost/performance targets while maintaining user-defined SLOs.
Aspiring to vLLM itself, we will have the most performant and featureful open source Intelligent router for AI workloads on Kubernetes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-what-are-the-key-features-of-llm-d-being-presented-at-red-hat-summit">Q: What are the key features of llm-d being presented at Red Hat Summit?<a href="#q-what-are-the-key-features-of-llm-d-being-presented-at-red-hat-summit" class="hash-link" aria-label="Direct link to Q: What are the key features of llm-d being presented at Red Hat Summit?" title="Direct link to Q: What are the key features of llm-d being presented at Red Hat Summit?">â€‹</a></h2>
<p><strong>A:</strong> At Red Hat Summit, we plan to showcase three key features: prefill/decode disaggregation, KV Cache aware routing, and an AI-aware inference-optimized router. These demos will highlight the performance benefits of these features, demonstrating improvements in Time-To-First-Token (TTFT), reduced latency, increased throughput, and improve resource utilization, leading to significant cost savings. Our goal is to show that both user experience (TTFT, latency, throughput) and cost-efficiency can be achieved simultaneously through a continuous stream of technical innovation from the llm-d community..</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-is-any-red-hat-subscription-required-to-use-llm-d">Q: Is any Red Hat subscription required to use llm-d?<a href="#q-is-any-red-hat-subscription-required-to-use-llm-d" class="hash-link" aria-label="Direct link to Q: Is any Red Hat subscription required to use llm-d?" title="Direct link to Q: Is any Red Hat subscription required to use llm-d?">â€‹</a></h2>
<p><strong>A:</strong> No. In fact we worked with CoreWeave to add support for CoreWeave Kubernetes Service (ubuntu) for launch!</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-we-dont-use-podman-can-we-still-use-llm-d">Q: We donâ€™t use podman. Can we still use llm-d?.<a href="#q-we-dont-use-podman-can-we-still-use-llm-d" class="hash-link" aria-label="Direct link to Q: We donâ€™t use podman. Can we still use llm-d?." title="Direct link to Q: We donâ€™t use podman. Can we still use llm-d?.">â€‹</a></h2>
<p><strong>A:</strong> Any CRI compatible container engine (such as docker-ce) will work, and we document it.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-we-dont-use-vllm-can-we-still-try-llm-d">Q: We donâ€™t use vLLM. Can we still try llm-d?<a href="#q-we-dont-use-vllm-can-we-still-try-llm-d" class="hash-link" aria-label="Direct link to Q: We donâ€™t use vLLM. Can we still try llm-d?" title="Direct link to Q: We donâ€™t use vLLM. Can we still try llm-d?">â€‹</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-while-llm-d-is-deeply-integrated-with-vllm-if-the-community-wanted-to-add-support-for-another-inference-engine-the-community-and-maintainers-would-review-it-as-any-proposal"><strong>A:</strong> While llm-d is deeply integrated with vLLM, if the community wanted to add support for another inference engine, the community and maintainers would review it as any proposal..<a href="#a-while-llm-d-is-deeply-integrated-with-vllm-if-the-community-wanted-to-add-support-for-another-inference-engine-the-community-and-maintainers-would-review-it-as-any-proposal" class="hash-link" aria-label="Direct link to a-while-llm-d-is-deeply-integrated-with-vllm-if-the-community-wanted-to-add-support-for-another-inference-engine-the-community-and-maintainers-would-review-it-as-any-proposal" title="Direct link to a-while-llm-d-is-deeply-integrated-with-vllm-if-the-community-wanted-to-add-support-for-another-inference-engine-the-community-and-maintainers-would-review-it-as-any-proposal">â€‹</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-what-is-an-ai-aware-inference-optimized-router">Q: What is an AI-aware inference-optimized router?<a href="#q-what-is-an-ai-aware-inference-optimized-router" class="hash-link" aria-label="Direct link to Q: What is an AI-aware inference-optimized router?" title="Direct link to Q: What is an AI-aware inference-optimized router?">â€‹</a></h2>
<p><strong>A:</strong> The router implementation in llm-d is one of its key features. We leverage the Gateway API Inference Extension for Kubernetes. As founding members of wg-serving and the inference extensions workstream, the group of launch partners behind llm-d have extended GAIE significantly during the bootstrapping of the llm-d project.. Our default implementation uses Envoy Proxy. The router will be the primary UX for a spectrum of software efficiency improvements (implemented as scheduler plugins) called scorers.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-what-is-a-scorer">Q: What is a scorer?<a href="#q-what-is-a-scorer" class="hash-link" aria-label="Direct link to Q: What is a scorer?" title="Direct link to Q: What is a scorer?">â€‹</a></h2>
<p><strong>A:</strong> Along with llm-d, we have also released a set of custom plugins that implement features like prefill/decode disaggregation. Other scorers optimize performance based on prefix, load or kv-cache locality. These scheduler components get loaded into the Gateway component of llm-d. An &quot;Endpoint Picker (EPP)&quot; binary and container images are provided which can be configured via Envoy&#x27;s ext-proc feature to make optimized routing decisions for AI Inference requests to backend model serving platforms (e.g. vLLM).</p>
<p>This functionality is built upon Gateway API and the Gateway API Inference Extension (GIE) projects for both the API resources and machinery, but extends support beyond what&#x27;s available in those projects by loading other custom plugins needed by llm-d (e.g. custom scorers, P/D Disaggregation, etc).
<strong>A:</strong> It looks at a prompt and decides what to do with it based on user desired state.</p>
<p>Because we know what type of traffic we will handle (we see L7), we can optimize the routerâ€™s behavior for inference.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-what-is-kv-cache-aware-routing-in-llm-d">Q: What is KV Cache Aware Routing in llm-d?<a href="#q-what-is-kv-cache-aware-routing-in-llm-d" class="hash-link" aria-label="Direct link to Q: What is KV Cache Aware Routing in llm-d?" title="Direct link to Q: What is KV Cache Aware Routing in llm-d?">â€‹</a></h2>
<p><strong>A:</strong> KV Cache Aware Routing in llm-d is a mechanism that optimizes the routing of inference requests based on the availability and reuse of the KV cache. The KV cache stores previously computed key and value attention vectors, which can significantly reduce computation time for subsequent requests, especially those sharing similar prefixes or context. By routing requests to nodes with existing relevant KV cache, llm-d aims to reduce Time-To-First-Token (TTFT) and improve overall performance. This routing is facilitated by a global KV cache index that maps cache blocks to their locations, allowing the router to make informed decisions about where to send each request. LMCache is involved as a KV cache engine.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-what-is-prefilldecode-disaggregation-in-llm-d">Q: What is prefill/decode disaggregation in llm-d?<a href="#q-what-is-prefilldecode-disaggregation-in-llm-d" class="hash-link" aria-label="Direct link to Q: What is prefill/decode disaggregation in llm-d?" title="Direct link to Q: What is prefill/decode disaggregation in llm-d?">â€‹</a></h2>
<p><strong>A:</strong> Prefill/decode disaggregation is a feature in llm-d that separates the inference process into two stages: prefill and decode. The prefill stage handles the initial processing of the input prompt, including tokenization and KV cache population. The decode stage generates the actual output tokens using the prefilled cache. This separation allows for optimized allocation of compute resources, as different types of hardware or configurations might be better suited for each stage. It aims to improve resource utilization, enable more efficient scheduling, and maintain high throughput and low latency, especially in heterogeneous environments. NIXL is used for KV transport in this process.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-what-is-lmcaches-role-in-llm-d">Q: What is LMCache&#x27;s role in llm-d?<a href="#q-what-is-lmcaches-role-in-llm-d" class="hash-link" aria-label="Direct link to Q: What is LMCache&#x27;s role in llm-d?" title="Direct link to Q: What is LMCache&#x27;s role in llm-d?">â€‹</a></h2>
<p><strong>A:</strong> LMCache acts as a KV Cache Engine within llm-d. Its role is to support KV cache offloading and sharing between vLLM nodes, which enhances performance by enabling reuse of previously computed KV caches. LMCache maintains a shared KV cache metadata store (Redis) that is used for KV Cache Indexing, allowing the system to track the location of cache blocks and optimize routing for KV cache hits. LMCache is currently the only open-source KVCache offloading/reusing engine and is crucial for enabling features like KV Cache Aware Routing in llm-d. It facilitates cross-node sharing by maintaining a global KV locality-index, enabling cross-node KVCache reuse. We spent time making these interfaces clean and pluggable.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q-where-do-i-submit-feedback-bug-reports-or-feature-requests">Q: Where do I submit feedback, bug reports or feature requests?<a href="#q-where-do-i-submit-feedback-bug-reports-or-feature-requests" class="hash-link" aria-label="Direct link to Q: Where do I submit feedback, bug reports or feature requests?" title="Direct link to Q: Where do I submit feedback, bug reports or feature requests?">â€‹</a></h2>
<p><strong>A:</strong> Check out the CONTRIBUTING.md. Weâ€™re using GitHub, so submit your issue or PR to the relevant repository.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/llm-d-website/docs/architecture/architecture"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Overview of llm-d architecture</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/llm-d-website/docs/architecture/Components/deployer"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Deployer Architecture</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#q-what-is-llm-d" class="table-of-contents__link toc-highlight">Q: What is llm-d?</a></li><li><a href="#q-what-is-the-goal-of-the-llm-d-project" class="table-of-contents__link toc-highlight">Q: What is the goal of the llm-d project?</a></li><li><a href="#q-what-is-the-engineering-design-philosophy-for-the-project" class="table-of-contents__link toc-highlight">Q: What is the engineering design philosophy for the project?</a></li><li><a href="#q-what-are-the-key-features-of-llm-d-being-presented-at-red-hat-summit" class="table-of-contents__link toc-highlight">Q: What are the key features of llm-d being presented at Red Hat Summit?</a></li><li><a href="#q-is-any-red-hat-subscription-required-to-use-llm-d" class="table-of-contents__link toc-highlight">Q: Is any Red Hat subscription required to use llm-d?</a></li><li><a href="#q-we-dont-use-podman-can-we-still-use-llm-d" class="table-of-contents__link toc-highlight">Q: We donâ€™t use podman. Can we still use llm-d?.</a></li><li><a href="#q-we-dont-use-vllm-can-we-still-try-llm-d" class="table-of-contents__link toc-highlight">Q: We donâ€™t use vLLM. Can we still try llm-d?</a></li><li><a href="#a-while-llm-d-is-deeply-integrated-with-vllm-if-the-community-wanted-to-add-support-for-another-inference-engine-the-community-and-maintainers-would-review-it-as-any-proposal" class="table-of-contents__link toc-highlight"><strong>A:</strong> While llm-d is deeply integrated with vLLM, if the community wanted to add support for another inference engine, the community and maintainers would review it as any proposal..</a></li><li><a href="#q-what-is-an-ai-aware-inference-optimized-router" class="table-of-contents__link toc-highlight">Q: What is an AI-aware inference-optimized router?</a></li><li><a href="#q-what-is-a-scorer" class="table-of-contents__link toc-highlight">Q: What is a scorer?</a></li><li><a href="#q-what-is-kv-cache-aware-routing-in-llm-d" class="table-of-contents__link toc-highlight">Q: What is KV Cache Aware Routing in llm-d?</a></li><li><a href="#q-what-is-prefilldecode-disaggregation-in-llm-d" class="table-of-contents__link toc-highlight">Q: What is prefill/decode disaggregation in llm-d?</a></li><li><a href="#q-what-is-lmcaches-role-in-llm-d" class="table-of-contents__link toc-highlight">Q: What is LMCache&#39;s role in llm-d?</a></li><li><a href="#q-where-do-i-submit-feedback-bug-reports-or-feature-requests" class="table-of-contents__link toc-highlight">Q: Where do I submit feedback, bug reports or feature requests?</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">User Guide</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d-website/docs/guide">How to Use</a></li></ul></div><div class="col footer__col"><div class="footer__title">Architecture</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d-website/docs/architecture">Overview</a></li><li class="footer__item"><a class="footer__link-item" href="/llm-d-website/docs/architecture/faq.md">FAQ</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d-website/docs/architecture/docs/community">How to Join in</a></li><li class="footer__item"><a class="footer__link-item" href="/llm-d-website/docs/architecture/docs/partners">Partners</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/llm-d-website/blog">News</a></li><li class="footer__item"><a href="https://github.com/llm-d/llm-d-website.github.io/tree/main/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></footer></div>
</body>
</html>